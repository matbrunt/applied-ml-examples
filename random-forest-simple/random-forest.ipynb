{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# modules for scaling, transforming, and wrangling data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# cross validation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# persist model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url, sep=';') # source data is ';' seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1599 samples, 12 features\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable (descriptor) is *quality*, which ranges from 3 to 8, but is mostly clustered around 4.8 to 6.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets, taking 20% of the data as a test set.\n",
    "\n",
    "We also stratify the sample by our target variable, which ensures that the training set looks similar to the test set, so the evaluation metrics should be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to standardise our features because they are on different scales. This involves centering the feature values around zero with roughly the same variance - we do this for each feature by subtracting its mean, then dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16664562e-16  -3.05550043e-17  -8.47206937e-17  -2.22218213e-17\n",
      "   2.22218213e-17  -6.38877362e-17  -4.16659149e-18  -2.54439854e-15\n",
      "  -8.70817622e-16  -4.08325966e-16  -1.17220107e-15]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "# Confirm scaled dataset is centered at zero, with unit variance\n",
    "print(X_train_scaled.mean(axis=0))\n",
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually do this when building the model because it introduces bias, when running the model against new data we won't have a representation of the data variance.\n",
    "\n",
    "So instead we use a pipeline with a *transformer*; this allows to calculate the means and std dev from the training set, then apply those same values to the test set. The transformer allows us to 'fit' a preprocessing step using training data the same way we would fit a model.\n",
    "\n",
    "This makes the model performance estimates more realistic, and allows us to use the preprocessing steps in **cross-validation pipeline**.\n",
    "\n",
    "We generate a scaler object with the saved means and standard deviations for each feature, then we can apply (transform) the exact same means and standard deviations using the scalar to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16664562e-16  -3.05550043e-17  -8.47206937e-17  -2.22218213e-17\n",
      "   2.22218213e-17  -6.38877362e-17  -4.16659149e-18  -2.54439854e-15\n",
      "  -8.70817622e-16  -4.08325966e-16  -1.17220107e-15]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Save the scaled means and standard deviations for each feature into a scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(X_train_scaled.mean(axis=0))\n",
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't need to manually fit the scaler, we can just pass the scaler class to use in the pipeline definition and it will automatically fit against the training data.\n",
    "\n",
    "Here we set up a **modeling pipeline** that first transforms the data with a StandardScalar(), then fits the model using a Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*model parameters* are learned directly from the data, i.e. they are regression co-efficients\n",
    "*hyperparameters* cannot be learned directly from the data, they express more high-level structural information about the model\n",
    "\n",
    "For example, Random Forest can use either MSE or MAE performance measures, but doesn't know which one to use. It also doesn't know how many trees it should grow against the data. These are both examples of hyperparameters that we must set, to give the model structural information on how to build itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'standardscaler__with_std': True, 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'standardscaler__copy': True, 'randomforestregressor__min_impurity_decrease': 0.0, 'randomforestregressor__random_state': None, 'randomforestregressor__verbose': 0, 'randomforestregressor__oob_score': False, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__n_jobs': 1, 'randomforestregressor__min_impurity_split': None, 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False), 'standardscaler__with_mean': True, 'randomforestregressor__min_samples_leaf': 1, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False))], 'memory': None, 'randomforestregressor__min_samples_split': 2, 'randomforestregressor__bootstrap': True, 'randomforestregressor__max_depth': None, 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__max_features': 'auto', 'randomforestregressor__warm_start': False, 'randomforestregressor__criterion': 'mse'}\n"
     ]
    }
   ],
   "source": [
    "# list the hyperparameters we can tune\n",
    "print(pipeline.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters in a pipeline are prefixed with their class instance, which you need to remember when setting them on the pipeline. For example, the RandomForestRegressor hyperparameters all begin with 'randomforestregressor__'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to reduce overfitting we want to take slices of our available data, and use K-fold cross validation to train the model on these different slices, and optimise the hyperparameters.\n",
    "\n",
    "GridSearchCV performs cross-validation across the entire grid of hyperparameters, e.g. all the possible permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decr...mators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1], 'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    "\n",
    "# fit and tune the model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "# look at the best set of parameters discovered using CV\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV will automatically refit the model with the best set of hyperparameters against the entire training set, this can be confirmed by checking that `clv.refit` reports `True`.\n",
    "\n",
    "Now we have our model (`clf`), which we can apply against other sets of data, such as the test set to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2:  0.470718566499\n",
      "----------------------------------------\n",
      "MSE:  0.34153125\n"
     ]
    }
   ],
   "source": [
    "def evaluate_performance(model, test_data, actual):\n",
    "    # make predictions against the test set\n",
    "    y_pred = model.predict(test_data)\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print('r2: ', r2_score(actual, y_pred))\n",
    "    print('-'*40)\n",
    "    print('MSE: ', mean_squared_error(actual, y_pred))\n",
    "    \n",
    "evaluate_performance(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model, we can persist it so we don't have to go through all these steps every time we see new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2:  0.470718566499\n",
      "----------------------------------------\n",
      "MSE:  0.34153125\n"
     ]
    }
   ],
   "source": [
    "# save model to a .pkl file\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "\n",
    "# load model from a .pkl file\n",
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    "\n",
    "evaluate_performance(clf2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
